Data Readiness Discovery,,,,,
,,,,,
Category,Question,Description,Customer [supported by Partner] responses,Additional Notes / Links,
Data Volume & Storage Requirements,What is the current volume of relevant data available for your GenAI application?,Quantifies data scale to determine processing and storage requirements.,"Assessment templates (JSON files), audio recordings from calls (5-10 files per assessment), and candidate transcripts averaging 2-5KB each.",Data volume scales with usage: estimated 1-5GB monthly for 500 assessments with full audio retention.,
,Where will the training data be stored?,Maps current and planned data repositories and access methods.,"AWS S3 buckets with structured organization: /assessments/{id}/ for recordings, /transcripts/ for processed text, /templates/ for assessment criteria.","Leverages S3's durability, versioning, and lifecycle management for cost-effective long-term storage.",
,What are the storage requirements for training data and model artifacts?,"Determines capacity, durability, and availability requirements.",High availability S3 storage with 99.999999999% durability; estimated 10-50GB annually with audio retention policies.,Model artifacts managed by AWS Bedrock; no local model storage required due to managed service approach.,
,What are the data retention and backup requirements?,Determines storage lifecycle management requirements.,"Audio recordings retained for 90 days, transcripts and analysis results kept for 2 years for compliance and performance tracking.",S3 lifecycle policies automatically transition older data to cheaper storage tiers and manage deletion schedules.,
,How frequently is your data updated or refreshed?,Assesses data currency requirements and pipeline update frequencies.,Assessment templates updated quarterly; candidate data generated in real-time during calls and processed within 10 minutes.,Static assessment criteria ensure consistency while allowing periodic improvements based on performance data.,
,How do you handle versioning and storage of different iterations of training datasets?,Evaluates version control practices and their impact on storage needs.,Assessment templates versioned in JSON with semantic versioning; S3 object versioning tracks all data changes automatically.,Version control enables rollback capabilities and A/B testing of different assessment criteria.,
,,,,,
Data Format & Preprocessing,"What data formats comprise your relevant data (structured, unstructured, semi-structured)?",Determines compatibility with GenAI processing requirements and necessary transformations.,Structured: JSON assessment templates and scoring data; Unstructured: Audio recordings (WAV/MP3) and transcribed text responses.,Well-defined data formats enable reliable processing pipeline with minimal transformation overhead.,
,Are your data sources centralized or distributed across multiple systems?,Identifies potential data integration challenges and accessibility issues.,"Centralized in AWS ecosystem: S3 for storage, Transcribe for speech-to-text, Bedrock for LLM analysis, Lambda for processing.",Single cloud provider reduces integration complexity and ensures consistent data access patterns.,
,What data pipeline infrastructure do you currently have in place?,Evaluates existing ETL/ELT capabilities that can support GenAI data flow.,Event-driven serverless pipeline: Twilio webhooks → Lambda → S3 → Transcribe → Lambda → Bedrock → S3 results storage.,Fully managed AWS services provide automatic scaling and built-in error handling for reliable data processing.,
,Can your current infrastructure support real-time data processing if required?,Determines if infrastructure upgrades are needed for latency-sensitive applications.,"Yes, Lambda functions provide real-time call handling; post-call analysis is asynchronous but completes within 5-10 minutes.",Architecture separates real-time user experience from batch processing for optimal performance and cost.,
,What languages are represented in your data?,Identifies multilingual requirements and potential challenges.,Currently English-only for US restaurant market; system designed to support additional languages as expansion requires.,AWS Transcribe and Bedrock Claude support multiple languages enabling future international deployment.,
,What text preprocessing or normalization steps have you applied to your data?,Evaluates text-specific preparation work completed for NLP applications.,"Minimal preprocessing required; AWS Transcribe handles audio normalization, LLM processes raw transcripts directly.",Leveraging managed services reduces need for custom preprocessing while maintaining high accuracy.,
,,,,,
AI-Specific Data Preparation,Do you have labeled datasets available for training or fine-tuning models?,Assesses availability of supervised learning data if model customization is required.,Using pre-trained AWS Bedrock Claude model; assessment templates provide structured evaluation criteria instead of training labels.,Approach leverages foundation model capabilities with domain-specific prompting rather than custom model training.,
,Have you performed any data profiling or exploratory analysis on your datasets?,Determines understanding of data characteristics that might impact AI performance.,"Analyzed call duration patterns, transcript length distributions, and response quality variations across different restaurant roles.","Data profiling informs timeout settings, question pacing, and expected response characteristics for optimal user experience.",
,Are there known biases or gaps in your data that could affect AI outputs?,Identifies potential ethical concerns and performance limitations to address.,Monitoring for potential bias in assessment scoring; diverse question set designed to evaluate skills rather than background.,Regular bias audits and diverse assessment criteria help ensure fair evaluation across different candidate demographics.,
,Have you established ground truth or benchmark datasets for evaluating AI performance?,Assesses readiness for measuring AI output quality and accuracy.,Establishing baseline through comparison with hiring manager evaluations and tracking employee performance correlation.,Ground truth development is ongoing with initial cohort of assessed candidates and their subsequent job performance.,
,Do you have mechanisms to monitor data drift or quality changes over time?,Identifies capabilities for maintaining AI performance as data evolves.,,"Monitoring assessment score distributions, transcript quality metrics, and correlation with hiring outcomes over time.",Automated alerts for significant changes in data patterns or assessment performance metrics.
,"Do you have processes in place to handle missing, incomplete, or inconsistent data?",Evaluates maturity of data cleansing procedures critical for reliable AI outputs.,"Built-in error handling for failed transcriptions, timeout management for non-responses, and fallback scoring for incomplete assessments.",Robust error handling ensures consistent evaluation even when technical issues or candidate behavior create data gaps.,
,What percentage of your data meets your quality standards for generative AI use?,Quantifies the proportion of data that is immediately usable versus requiring cleanup,Approximately 85% of call recordings produce high-quality transcripts; 95% of assessments generate usable analysis results.,Quality standards focus on transcript accuracy and completeness rather than perfect audio quality for practical deployment.,
,,,,,
Data Quality & Governance,How do you ensure and measure the quality of your data?,Assesses existing data quality processes and metrics that would impact GenAI performance.,"Audio quality monitoring, transcription accuracy validation, and assessment result consistency checks with manual review sampling.",Automated quality metrics track transcription confidence scores and flag assessments requiring human review.,
,How do you classify and manage sensitive information within your datasets?,Assesses risk management practices for personally identifiable or confidential information.,"Candidate phone numbers and personal responses treated as PII; encrypted storage, access controls, and retention policies implemented.","GDPR/CCPA compliance through data minimization, consent tracking, and right-to-deletion capabilities.",
,Do you have documented data lineage for the sources you plan to use?,Determines traceability and auditability of data feeding into the GenAI system.,Complete audit trail: Twilio call records → S3 storage → Transcribe processing → LLM analysis → final assessment results.,"Each assessment includes metadata tracking processing timestamps, service versions, and data transformation steps.",
,What data access controls and permissions are currently in place?,Identifies security measures and potential constraints for AI system access to data.,"IAM role-based access, S3 bucket policies, and Lambda function permissions restrict data access to authorized services only.",Principle of least privilege ensures each component can only access data required for its specific function.,
,Who owns and manages the data sources you plan to use?,Identifies data stewards and access approval processes.,GravyWork owns assessment templates and system data; candidate data owned by respective restaurants with clear usage agreements.,"Data governance framework defines ownership, access rights, and usage permissions across all stakeholders.",
,How do you plan to build trust about data usage in generative AI among stakeholders?,Evaluates transparency and communication strategies regarding AI data usage.,"Transparent scoring criteria, explainable AI results with reasoning, and clear data usage policies communicated to all users.","Regular stakeholder updates on system performance, bias monitoring results, and continuous improvement efforts.",